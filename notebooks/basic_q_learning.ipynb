{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimize Q lookup table and visualize policy\n",
    "## Overview\n",
    "This notebook creates an environment with a smoke plume movie and an agent that learns to seek odor sources using RL. It's useful to see how the relevant classes are created. Q values for state-action pairs are optimized in loop in the notebook, so it also demonstrates this simple learning rule.\n",
    "## Arena\n",
    "The agent navigates an arena that is 'odorized' according to a smoke plume movie ('project_root/src/data/plume_movies/intermiitent_smoke.avi').\n",
    "## State space\n",
    "The agent detects:\n",
    "    - odor concentration (binarized into low and high)\n",
    "    - odor gradient in the cross-wind direction (discretized into crosswind A, crosswind B, neither)\n",
    "    - and odor motion in the cross-wind direction (crosswind A, crosswind B, neither).\n",
    "## Action space\n",
    "The agent walks with constant speed and chooses a walking direction at every timestep. The walking directions are the upwind/downwind/crosswind A/crosswind B directions, as well as the intermediate directions (i.e., at 45 degree angles).\n",
    "## Reward\n",
    "The agent receives a reward quantity at every timepoint. This is 1 on timesteps where the agent gets to the goal zone; otherwise it is 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tqdm.notebook\n",
    "\n",
    "from src.models.goals import GOAL_X,GOAL_Y,GOAL_RADIUS\n",
    "from src.models.motion_environment_factory import PlumeMotionNavigationEnvironmentMovie1PlumeSourceRewardFactory\n",
    "\n",
    "N_EPISODES = 2000 # How many independently initialized runs to train on\n",
    "ALPHA = 0.1 # Learning rate\n",
    "GAMMA = 0.5 # Reward temporal discount factor\n",
    "MAX_EPSILON = 1 # Starting exploration rate\n",
    "MIN_EPSILON = 0.1 # Asymptote of decaying exploration rate\n",
    "DECAY = 0.1 # Rate of exploration decay\n",
    "\n",
    "MIN_RESET_X = GOAL_X + 10 + GOAL_RADIUS # Initialization condition\n",
    "MAX_RESET_X = 1430 # Initialization condition\n",
    "rewards = np.zeros(N_EPISODES)\n",
    "total_rewards = 0\n",
    "\n",
    "\n",
    "plume_movie_path = os.path.join('..', 'src', 'data', 'plume_movies', 'intermittent_smoke.avi')\n",
    "environment = PlumeMotionNavigationEnvironmentMovie1PlumeSourceRewardFactory(movie_file_path=plume_movie_path).plume_environment\n",
    "q_shape = np.append(environment.observation_space.nvec,environment.action_space.n)\n",
    "q_table = np.zeros(shape=q_shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q_SAVE_NAME = 'q_table_YY_MM_DD.npy'\n",
    "\n",
    "save_path = os.path.join('..','trained_models',Q_SAVE_NAME)\n",
    "\n",
    "epsilon = MAX_EPSILON\n",
    "alpha = ALPHA\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "min_reset_x = MIN_RESET_X\n",
    "max_reset_x = min_reset_x + GOAL_RADIUS\n",
    "print(f'starting less than {max_reset_x} away in x coordinate')\n",
    "reset_y_radius = 400\n",
    "transition_incrementer = 0\n",
    "parameter_decay = 0\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "for episode in tqdm.notebook.tqdm(range(N_EPISODES)):\n",
    "    if (transition_incrementer > 0) & (transition_incrementer % 10 == 0):\n",
    "        transition_incrementer -= 10\n",
    "        old_reset_x = max_reset_x\n",
    "        old_reset_y = reset_y_radius\n",
    "        max_reset_x = np.min([MAX_RESET_X,GOAL_RADIUS+max_reset_x])\n",
    "\n",
    "        if (old_reset_x != min_reset_x):\n",
    "\n",
    "            print('Making task harder')\n",
    "            print(f'starting less than {max_reset_x} away in x coordinate')\n",
    "            parameter_decay -= 3 # Increase exploration rate a bit following the change in initialization conditions\n",
    "\n",
    "    flip = np.random.choice([True,False],1)\n",
    "    observation = environment.reset(options={'randomization_x_bounds':np.array([min_reset_x,min_reset_x+GOAL_RADIUS]),\n",
    "                                             'randomization_y_bounds': np.array([-reset_y_radius, reset_y_radius]) + GOAL_Y,\n",
    "                                             'flip':flip})\n",
    "\n",
    "    done = False\n",
    "    while not done: # Advance the environment (e.g., the smoke plume updates and the agent walks a step)\n",
    "        explore = rng.random() < epsilon# Can pick all random numbers at start\n",
    "        if explore:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[tuple(observation)])\n",
    "\n",
    "        new_observation, reward, done, odor_measures = environment.step(action)\n",
    "        if reward > 0:\n",
    "            transition_incrementer += 1\n",
    "            parameter_decay +=1\n",
    "            total_rewards += 1\n",
    "            print('received reward')\n",
    "        update_index = tuple(np.append(observation,action))\n",
    "        t1_value_index = tuple(new_observation)# Note the use of this index requires actions to be last axes of q table\n",
    "        q_table[update_index] = \\\n",
    "            q_table[update_index] +\\\n",
    "            ALPHA * (reward + GAMMA*np.max(q_table[t1_value_index]) -\\\n",
    "            q_table[update_index])\n",
    "        observation = new_observation\n",
    "\n",
    "\n",
    "        epsilon = MIN_EPSILON + (MAX_EPSILON-MIN_EPSILON)*np.exp(-DECAY*parameter_decay)\n",
    "np.save(save_path,q_table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\n",
    "Below, save frames of a movie depicting the trained agent walking through a smoke plume according its learned policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import src.visualization.visualize_behavior\n",
    "\n",
    "POLICY_FRAMES_PARENT_DIRECTORY = 'q_policy_example_frames_YY_MM_DD'\n",
    "\n",
    "policy_movie_path = os.path.join('..','result_images',POLICY_FRAMES_PARENT_DIRECTORY)\n",
    "\n",
    "src.visualization.visualize_behavior.main(movie_path=plume_movie_path,q_table_path=save_path,savepath=policy_movie_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
